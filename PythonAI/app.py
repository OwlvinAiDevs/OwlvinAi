import logging
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from ai_model import generate_schedule, format_schedule_prompt, format_chat_prompt, call_openai_api
from utils import parse_llm_response
from models import StudyRequest, ScheduleResponse

logging.basicConfig(level=logging.DEBUG)

# Initialize the FastAPI app
app = FastAPI()

@app.get("/ping")
def ping():
    """A simple endpoint to check if the server is running."""
    return {"message": "pong"}

@app.post("/generate_schedule", response_model=ScheduleResponse)
def schedule(request: StudyRequest):
    """
    Generates a schedule using a deterministic, rule-based engine.
    This endpoint is stateless and purely computational.
    """
    logging.info(f"Received Rule-Based StudyRequest: user_id={request.user_id}")
    # Basic validation of the request payload
    if not request.available_slots:
        raise HTTPException(status_code=400, detail="No available time slots provided.")
    if not request.tasks:
        raise HTTPException(status_code=400, detail="No tasks provided for scheduling.")
    return generate_schedule(request)

@app.post("/generate_ai_schedule", response_model=ScheduleResponse)
async def generate_ai_schedule(request: StudyRequest):
    """
    Generates a schedule using the OpenAI API.
    This endpoint is stateless and falls back to the rule-based engine on failure.
    """
    try:
        logging.info(f"[START] /generate_ai_schedule for user_id={request.user_id}")
        prompt = format_schedule_prompt(request)
        gpt_response = await call_openai_api(prompt)
        
        if not isinstance(gpt_response, list):
            raise ValueError("Invalid response format from OpenAI API.")

        sessions = parse_llm_response(gpt_response)
        logging.info(f"Parsed {len(sessions)} sessions from GPT response")

        # Calculate metrics and format the response to send back to the client
        total_study_time = sum(s.task.duration_minutes for s in sessions)
        total_break_time = sum(s.break_after for s in sessions if s.break_after)
        scheduled_titles = {s.task.title for s in sessions}
        unscheduled_tasks = [t for t in request.tasks if t.title not in scheduled_titles]
        warnings = [f"AI did not schedule task: '{t.title}'" for t in unscheduled_tasks]

        logging.info(f"[DONE] Schedule generated with {len(warnings)} warnings")
        return ScheduleResponse(
            user_id=request.user_id,
            sessions=sessions,
            total_study_time=total_study_time,
            total_break_time=total_break_time,
            success=not unscheduled_tasks,
            message="Schedule generated by AI.",
            warnings=warnings
        )
    except Exception as e:
        logging.warning(f"[FALLBACK] AI scheduling failed: {e}. Using rule-based scheduling.")
        fallback_response = generate_schedule(request)
        fallback_response.warnings.append("AI scheduling failed. Using rule-based fallback.")
        fallback_response.success = False
        return fallback_response

class ChatPrompt(BaseModel):
    """The data model for a chat request from the client."""
    user_id: int
    message: str
    context: Optional[str] = "" # Context is provided by the client

@app.post("/chat")
async def chat(prompt: ChatPrompt):
    """
    Handles chat requests by forwarding them to the OpenAI API.
    This endpoint is stateless.
    """
    try:
        final_prompt = format_chat_prompt(prompt.message, prompt.context)
        gpt_response = await call_openai_api(final_prompt)
        return {"response": gpt_response}
    except Exception as e:
        logging.error(f"[CHAT ERROR] {e}")
        raise HTTPException(status_code=500, detail="Error processing chat request")
